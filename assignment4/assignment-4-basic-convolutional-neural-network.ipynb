{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Multiclass Classification using Keras and TensorFlow on Food-101 Dataset**\n![alt text](https://www.vision.ee.ethz.ch/datasets_extra/food-101/static/img/food-101.jpg)","metadata":{"_uuid":"41a6777d5e67dc652f57ce9681b4c44dc44152be","id":"uNaVQGQ9tQRr"}},{"cell_type":"markdown","source":"### **Download and extract Food 101 Dataset**","metadata":{"_uuid":"201bfa1a371439fbb44da6ef4e8d232bcca0b465","id":"Xa07tVPbP7Cu"}},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.image as img\n%matplotlib inline\nimport numpy as np\nfrom collections import defaultdict\nimport collections\nfrom shutil import copy\nfrom shutil import copytree, rmtree\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nimport cv2","metadata":{"_uuid":"dd82702380162e9587a0eae2f644dae2764f93c8","execution":{"iopub.status.busy":"2022-05-03T21:48:57.819311Z","iopub.execute_input":"2022-05-03T21:48:57.81972Z","iopub.status.idle":"2022-05-03T21:49:00.553887Z","shell.execute_reply.started":"2022-05-03T21:48:57.819665Z","shell.execute_reply":"2022-05-03T21:49:00.553203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is enabled\nprint(tf.__version__)\nprint(tf.test.gpu_device_name())","metadata":{"_uuid":"70f06e9a535b5f32ad9d927fc00e767dd72f17dd","id":"JOZZbCDoP-Hy","outputId":"99f6277e-0b8e-4541-a9b4-cce1a98f5b57","execution":{"iopub.status.busy":"2022-05-03T21:49:00.556037Z","iopub.execute_input":"2022-05-03T21:49:00.556488Z","iopub.status.idle":"2022-05-03T21:49:01.661978Z","shell.execute_reply.started":"2022-05-03T21:49:00.556431Z","shell.execute_reply":"2022-05-03T21:49:01.661209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/input/food-101/","metadata":{"_uuid":"c134497b2239d3fa5f377fb5aebf1887f627233f","execution":{"iopub.status.busy":"2022-05-03T21:49:01.665022Z","iopub.execute_input":"2022-05-03T21:49:01.66535Z","iopub.status.idle":"2022-05-03T21:49:01.681054Z","shell.execute_reply.started":"2022-05-03T21:49:01.6652Z","shell.execute_reply":"2022-05-03T21:49:01.679309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to download data and extract\ndef get_data_extract():\n  if \"food-101\" in os.listdir():\n    print(\"Dataset already exists\")\n  else:\n    print(\"Downloading the data...\")\n    !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n    print(\"Dataset downloaded!\")\n    print(\"Extracting data..\")\n    !tar xzvf food-101.tar.gz\n    print(\"Extraction done!\")","metadata":{"_uuid":"339ff95b36c15cfb16a9316d2a19cc89aaaf7160","id":"f88XvEBTQBS9","execution":{"iopub.status.busy":"2022-05-03T21:49:01.682116Z","iopub.execute_input":"2022-05-03T21:49:01.682378Z","iopub.status.idle":"2022-05-03T21:49:01.688316Z","shell.execute_reply.started":"2022-05-03T21:49:01.682324Z","shell.execute_reply":"2022-05-03T21:49:01.687199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Commented the below cell as the Food-101 dataset is available from Kaggle Datasets and need not be downloaded..**","metadata":{"_uuid":"c36f8e6e00be58a643a598469cc3fd9a8c47a912","id":"gwLp2G9ae9xC"}},{"cell_type":"code","source":"# Download data and extract it to folder\n# Uncomment this below line if you are on Colab\n\n#get_data_extract()","metadata":{"_uuid":"05fd6dae2dde446b3bd4a17f81a202391f9328d3","id":"O7kY0v23QJGO","outputId":"edc14855-bf55-4938-a875-af7d24729ea6","execution":{"iopub.status.busy":"2022-05-03T21:49:01.689492Z","iopub.execute_input":"2022-05-03T21:49:01.689862Z","iopub.status.idle":"2022-05-03T21:49:01.697329Z","shell.execute_reply.started":"2022-05-03T21:49:01.689742Z","shell.execute_reply":"2022-05-03T21:49:01.696547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Understand dataset structure and files**","metadata":{"_uuid":"5f65273b9b9b58da245745c82ac6cc5dfa54eeda","id":"eQr6hmptQe6q"}},{"cell_type":"markdown","source":"**The dataset being used is [Food 101](https://www.vision.ee.ethz.ch/datasets_extra/food-101/)**\n* **This dataset has 101000 images in total. It's a food dataset with 101 categories(multiclass)**\n* **Each type of food has 750 training samples and 250 test samples**\n* **Note found on the webpage of the dataset :  **  \n***On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.***  \n* **The entire dataset is 5GB in size**","metadata":{"_uuid":"40ac5b4a09f980a6a40dc953ed0c2b35b2439552","id":"n0xi2zwVQsWq"}},{"cell_type":"markdown","source":"### **Split the image data into train and test using train.txt and test.txt**","metadata":{"_uuid":"7a8300cde1146c50e672079b44346e723d813702","id":"KIgareCETmct"}},{"cell_type":"code","source":"# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src,dest):\n  classes_images = defaultdict(list)\n  with open(filepath, 'r') as txt:\n      paths = [read.strip() for read in txt.readlines()]\n      for p in paths:\n        food = p.split('/')\n        classes_images[food[0]].append(food[1] + '.jpg')\n\n  for food in classes_images.keys():\n#     print(\"\\nCopying images into \",food)\n    if not os.path.exists(os.path.join(dest,food)):\n      os.makedirs(os.path.join(dest,food))\n    for i in classes_images[food]:\n      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n  print(\"Copying Done!\")","metadata":{"_uuid":"d6c87e52c1af18e3243f1cb72b5834941828b286","id":"xB0XMUX_5KMQ","execution":{"iopub.status.busy":"2022-05-03T21:49:01.699056Z","iopub.execute_input":"2022-05-03T21:49:01.699575Z","iopub.status.idle":"2022-05-03T21:49:01.708455Z","shell.execute_reply.started":"2022-05-03T21:49:01.699355Z","shell.execute_reply":"2022-05-03T21:49:01.707431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n%cd /\nprint(\"Creating train data...\")\nprepare_data('/kaggle/input/food-101/food-101/meta/train.txt', '/kaggle/input/food-101/food-101/images', 'train')","metadata":{"_uuid":"786cdb3d60ebcdddb4c2875f9e0b4508c5210fc1","id":"LSgcYcqy5KUd","outputId":"7e65498b-bcd8-4209-87e8-bdc1a8c37b4e","execution":{"iopub.status.busy":"2022-05-03T21:49:01.709956Z","iopub.execute_input":"2022-05-03T21:49:01.710429Z","iopub.status.idle":"2022-05-03T21:58:58.725797Z","shell.execute_reply.started":"2022-05-03T21:49:01.71022Z","shell.execute_reply":"2022-05-03T21:58:58.725001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\nprint(\"Creating test data...\")\nprepare_data('/kaggle/input/food-101/food-101/meta/test.txt', '/kaggle/input/food-101/food-101/images', 'test')","metadata":{"_uuid":"ba85577e0ef10c768e000ecf32dee36566d52599","id":"JI65wZgT5Kb-","outputId":"2e71e6bc-43de-4ea3-f5d6-a660c4a3dc42","execution":{"iopub.status.busy":"2022-05-03T21:58:58.726993Z","iopub.execute_input":"2022-05-03T21:58:58.727479Z","iopub.status.idle":"2022-05-03T22:02:21.271402Z","shell.execute_reply.started":"2022-05-03T21:58:58.727206Z","shell.execute_reply":"2022-05-03T22:02:21.270402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many files are in the train folder\nprint(\"Total number of samples in train folder\")\n!find train -type d -or -type f -printf '.' | wc -c","metadata":{"_uuid":"b5a96c42b2af54aa994b86a58d2657b81786381b","id":"Xccc8PJP5K1G","outputId":"981ab583-491f-41ff-a128-d1f29c137775","execution":{"iopub.status.busy":"2022-05-03T22:02:21.275861Z","iopub.execute_input":"2022-05-03T22:02:21.276463Z","iopub.status.idle":"2022-05-03T22:02:22.047507Z","shell.execute_reply.started":"2022-05-03T22:02:21.276116Z","shell.execute_reply":"2022-05-03T22:02:22.046722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many files are in the test folder\nprint(\"Total number of samples in test folder\")\n!find test -type d -or -type f -printf '.' | wc -c","metadata":{"_uuid":"492bb30a200d6cde501fbe2a6424c1c05a67c2e8","id":"Iz3fjQw25K3-","outputId":"b667062f-9acb-4be7-81ff-15347abdc750","execution":{"iopub.status.busy":"2022-05-03T22:02:22.049299Z","iopub.execute_input":"2022-05-03T22:02:22.049591Z","iopub.status.idle":"2022-05-03T22:02:22.803803Z","shell.execute_reply.started":"2022-05-03T22:02:22.049542Z","shell.execute_reply":"2022-05-03T22:02:22.802963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Fine tune Inception Pretrained model using Food 101 dataset**","metadata":{"_uuid":"3a9e9afbfe1fc303c23c06c27444b66988ab8e9d","id":"upx61ukJiA8B"}},{"cell_type":"markdown","source":"* Keras and other Deep Learning libraries provide pretrained models  \n* These are deep neural networks with efficient architectures(like VGG,Inception,ResNet) that are already trained on datasets like ImageNet  \n* Using these pretrained models, we can use the already learned weights and add few layers on top to finetune the model to our new data  \n* This helps in faster convergance and saves time and computation when compared to models trained from scratch","metadata":{"_uuid":"b9b7bc7c0a9ca43a9b0a2d1c77e11bd40f305ae9","id":"z5hh8fj8iIaV"}},{"cell_type":"markdown","source":"* We currently have a subset of dataset with 3 classes - samosa, pizza and omelette  \n* Use the below code to finetune Inceptionv3 pretrained model","metadata":{"_uuid":"131cda47a01c9a62a8c9563c74a2b8b3545e392a","id":"8AzJVmphi0VQ"}},{"cell_type":"code","source":"K.clear_session()\nn_classes = 101\nimg_width, img_height = 299, 299\ntrain_data_dir = 'train'\nvalidation_data_dir = 'test'\nnb_train_samples = 75750\nnb_validation_samples = 25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n\n# predictions = Dense(101,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(299, 299, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(101))\n\nmodel.compile(optimizer='adam',\n#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping()\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples // batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples // batch_size,\n                    epochs=20,\n                    verbose=1,\n                    callbacks=[early_stopping])\n\nmodel.save('model_trained_101class.hdf5')\n","metadata":{"_uuid":"8d08ece78ab731f7ac8a9b4b581e42e29093bcca","id":"JBs1U7hZkp1U","outputId":"83b079b8-2550-41db-fcac-961fb11c07fc","execution":{"iopub.status.busy":"2022-05-03T22:02:22.807156Z","iopub.execute_input":"2022-05-03T22:02:22.807432Z","iopub.status.idle":"2022-05-03T23:24:54.311215Z","shell.execute_reply.started":"2022-05-03T22:02:22.807374Z","shell.execute_reply":"2022-05-03T23:24:54.310471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')\n\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:24:54.312513Z","iopub.execute_input":"2022-05-03T23:24:54.312768Z","iopub.status.idle":"2022-05-03T23:24:54.663742Z","shell.execute_reply.started":"2022-05-03T23:24:54.312725Z","shell.execute_reply":"2022-05-03T23:24:54.533432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_map_3 = train_generator.class_indices\nclass_map_3","metadata":{"_uuid":"558180f917fba895b9792328b234421c097f1ba0","execution":{"iopub.status.busy":"2022-05-03T23:24:54.534109Z","iopub.status.idle":"2022-05-03T23:24:54.534679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Visualize the accuracy and loss plots**","metadata":{"_uuid":"732521c21ce35b5be0d571cbe9d392d93755671c","id":"KbDzLAHGpJXQ"}},{"cell_type":"code","source":"def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_accuracy(history,'FOOD101-Inceptionv3')\nplot_loss(history,'FOOD101-Inceptionv3')","metadata":{},"execution_count":null,"outputs":[]}]}